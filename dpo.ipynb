{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def dpo_loss_bradley_terry(logp_ref, logp_train, x, y_win, y_lose, beta=0.1):\n",
    "    return -torch.nn.functional.logsigmoid(\n",
    "        beta * (logp_train(x, y_win) - logp_ref(x, y_win))\n",
    "        - beta * (logp_train(x, y_lose) - logp_ref(x, y_lose))\n",
    "    )\n",
    "\n",
    "def dpo_loss_plackett_luce(logp_ref, logp_train, x, y_best_to_worst, beta=0.1):\n",
    "    N = len(y_best_to_worst)\n",
    "    reward = torch.empty(N, dtype=torch.float32)\n",
    "    log_softmaxes = torch.empty(N, dtype=torch.float32)\n",
    "    for i, y in enumerate(y_best_to_worst):\n",
    "        reward[-i] = beta * (logp_train(x, y) - logp_ref(x, y))\n",
    "        log_softmaxes[-i] = torch.nn.functional.log_softmax(reward[-i:])[0]\n",
    "    return -torch.sum(log_softmaxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Off-policy setup: fine-tune model until close to reference model, then use on-policy setup\n",
    "# Interactive setup: run policy to get rankings, then use on-policy setup\n",
    "def interactive_dpo(ref, train, prompts):\n",
    "    dataset = []\n",
    "    for x in prompts:\n",
    "        y0, y1 = ref(x), ref(x)\n",
    "        winner = int(input(f\"Which is better? 0:{y0} or 1:{y1}?\"))\n",
    "        y_win, y_lose = y0, y1 if winner == 0 else y1, y0\n",
    "        dataset.append((x, y_win, y_lose))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On-policy setup\n",
    "# x: tokenized prompt, y: continuation\n",
    "def logp_model(model):\n",
    "    def logp(x, y):\n",
    "        # Concat in the sequence dimension\n",
    "        logits = model(torch.cat((x, y), -2)).logits()\n",
    "        # Regularize the logits in the vocabulary dimension. Skip the logits of the prompt, accounting for that\n",
    "        # logits are not output for the first token.\n",
    "        regularized = torch.nn.functional.log_softmax(logits[:, x.shape[1]-1:, :], -1)\n",
    "        # Sum logits over the sequence dimension to get logp of the entire output.\n",
    "        return torch.sum(regularized, -2)\n",
    "    return logp\n",
    "\n",
    "def on_policy_dpo(ref, train, dataset):\n",
    "    # Ref should be frozen, typically a copy of the model to be trained.\n",
    "    for param in ref.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    logp_ref = logp_model(ref)\n",
    "    logp_train = logp_model(train)\n",
    "    optimizer = torch.optim.AdamW(train.parameters())\n",
    "    for x, y_win, y_lose in dataset:\n",
    "        loss = dpo_loss_bradley_terry(logp_ref, logp_train, x, y_win, y_lose)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
